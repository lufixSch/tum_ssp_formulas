\section{Estimation}
\begin{mdframed}[style=eqbox]
  \subsection{Estimator Quality}
  Consistency: $\lim_{n \rightarrow \infty} P(\mid \hat{\theta}_n - \theta \mid > \epsilon) = 0$\\
  Unbiasedness: $E[\hat{\theta}] = \theta$ with $\text{Bias}[\hat{\theta}] = E[\hat{\theta}] - \theta$\\
  Variance: $\text{Var}[T] = E[(T - E[T])^2]$
  \subsubsection{Mean Squared Error (MSE)}
  The MSE is the expected value of the squared error.
  \begin{mdframed}[style=redbox]
    \vspace*{-10pt}
    \begin{align*}
      \epsilon[T] &= E[(T - \theta)^2] = \text{Var}[T] + \text{Bias}^2[T]\\
      \epsilon[\underbar{T}] &= \text{E}[\mid\mid \underbar{T} - \underbar{\theta} \mid\mid^2] = \text{tr}\{E[(\underbar{T} - \underbar{\theta})(\underbar{T} - \underbar{\theta})^T]\}
    \end{align*}
  \end{mdframed}
  \subsubsection{Minimum Mean Squared Error (MMSE)}
  The MMSE is the minimum MSE over all possible estimators.
  \begin{align*}
    \arg \min_T E[(T - \theta)^2]
  \end{align*}
  An estimator is MMSE if it minimizes the MSE for all $\theta$.
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Maximum Likelihood Estimation (ML)}
  The ML estimator is the value of $\theta$ that maximizes the likelihood function $L(x;\theta)$ given $f_X(x; \theta)$.\\[0.25em]
  \textbf{Likelihood function:}\\
  $L(x_1, \ldots, x_n; \theta) = f_{X_1, \ldots, X_n}(x_1, \ldots, x_n; \theta)$\\
  $L(x_1, \ldots, x_n; \theta) = P_\theta(X_1 = x_1, \ldots, X_n = x_n)$\\[0.5em]
  If $N$ observations are i.i.d.:
  \vspace*{-4pt}
  \begin{align*}
    L(x;\theta) = \prod_{i=1}^N f_{X_i}(x_i; \theta) && l(x;\theta) = \sum_{i=1}^N \log f_{X_i}(x_i; \theta)
  \end{align*}
  \textbf{Maximum Likelihood Estimator:}
  \vspace*{-4pt}
  \begin{align*}
    T_{ML} = \arg \max_\theta \{ L(x;\theta) \} &= \arg\max_\theta \{ l(x;\theta)\}\\
    \frac{\delta L(x;\theta)}{\delta \theta} &= \frac{\delta l(x;\theta)}{\delta \theta} \overset{!}{ =} 0
  \end{align*}
  \textbf{Properties:}
  The ML Estimator is consistent, asymptotically unbiased and asymptotically efficient.
\end{mdframed}
\newpage

% Uniformly Minimum Variance Unbiased Estimator (UMVU)
\begin{mdframed}[style=eqbox]
  \subsection{Uniformly Minimum Variance Unbiased Estimator (UMVU)}
  The UMVU estimator is the unbiased estimator with the smallest variance. (Best unbiased estimator)\\[0.25em]
  \textbf{Fisher's Information Inequality:} Estimate lower bound for the variance if\\
  $ L(x, \theta) > 0 \quad \forall x, \theta$\\[0.25em]
  $ L(x, \theta)$ is twice differentiable in $\theta$\\[0.25em]
  $ \frac{\delta}{\delta \theta} \int L(x, \theta) \text{d}x = \int \frac{\delta}{\delta \theta} L(x, \theta) \text{d}x$\\[0.5em]
  \textbf{Score function:}
  \vspace*{-4pt}
  \begin{align*}
    g(x, \theta) = \frac{\delta}{\delta \theta} \log L(x, \theta) = \frac{\frac{\delta}{\delta \theta} L(x, \theta)}{L(x, \theta)} && E[g(x, \theta)] = 0
  \end{align*}
  \textbf{Fisher information:}
  \vspace*{-4pt}
  \begin{align*}
    I_F(\theta) := Var[g(X, \theta)] = E[g(x, \theta)^2] = -E\left[\frac{\delta^2}{\delta \theta^2} \log L(x, \theta)\right]
  \end{align*}
  \textbf{Cramer-Rao Lower Bound (CRB):}
  \vspace*{-4pt}
  \begin{mdframed}[style=redbox]
    \vspace*{-10pt}
    \begin{align*}
      \text{Var}[T] \geq \left ( \frac{\delta E[T(X)]}{\delta \theta} \right )^2 \frac{1}{I_F(\theta)} && \text{Var}[T] \geq \frac{1}{I_F(\theta)}
    \end{align*}
  \end{mdframed}
  \vspace*{-4pt}\small{with $T$ being unbiased $\implies E[T(X)] = \theta$}\\[0.25em]
  For $N$ i.i.d. observations: $I_F^{(N)}(x, \theta) = N \cdot I_F(x, \theta)$
  \subsubsection{Exponential Models}
  \vspace*{-4pt}
  \begin{align*}
    f_X(x; \theta) = \frac{h(x)\exp(a(\theta)t(x))}{\exp(b(\theta))} && I_F(\theta) = \frac{\delta a}{\delta \theta} \frac{\delta E[t(X)]}{\delta \theta}
  \end{align*}
  \subsubsection{Useful derivations}
  Uniform $\mathcal{U}(a,b)$: Not differentiable $\implies$ no $I_F(\theta)$\\[0.25em]
  Normal $\mathcal{N}(\mu, \sigma^2)$: $g(x,\theta) = \frac{x - \theta}{\sigma^2}$ $I_F(\theta) = \frac{1}{\sigma^2}$\\[0.25em]
  Binomial $\mathcal{B}(K, \theta)$: $g(x,\theta) = \frac{x}{\theta} - \frac{K-x}{1-\theta}$ $I_F(\theta) = \frac{K}{\theta(1-\theta)}$\\[0.25em]
\end{mdframed}

% Bayes Estimation (Conditional Mean)
\begin{mdframed}[style=eqbox]
  \subsection{Bayes Estimation (Conditional Mean)}
  A priori information about $\theta$ is given by the pdf $f_\Theta(\theta; \sigma)$. The conditional pdf (posterior pdf) $f_{X\mid\Theta}(x\mid\theta)$ is used to find $\theta$ by minimizing the mean MSE instead of the uniform MSE.\\[0.25em]
  Mean MSE for $\Theta$: $E[E[(T(X) - \Theta)^2 \mid \Theta = \theta]]$ \\[0.25em]
  \textbf{Conditional Mean Estimator:}
  \vspace*{-4pt}
  \begin{align*}
    T_{CM} : x &\mapsto E[\Theta \mid X = x] = \int \theta f_{\Theta \mid X}(\theta \mid x) \text{d}\theta\\
    f_{\Theta \mid X}(\theta \mid x) &= \frac{f_{X \mid \Theta}(x \mid \theta) f_\Theta(\theta)}{\int_\Theta f_{X, \theta}(x, \theta)\text{d}\theta} = \frac{f_{X \mid \Theta}(x \mid \theta) f_\Theta(\theta)}{f_X(x)}\\
  \end{align*}\vspace*{-24pt}\\
  \small{with $f_X(x) = \text{const.} \implies$ can be replaced by a factor $\frac{1}{\gamma}$. $\gamma$ can be determined such that $\int_\Theta f_{\Theta \mid X}(\theta \mid x)\text{d}\theta = 1$}\\[0.5em]
  MSE if $\Theta \sim \mathcal{N}(\mu, \sigma_\Theta^2)$, $X \sim \mathcal{N}(\mu, \sigma_X^2)$: $E[\text{Var}[\Theta \mid X]]$ \\[0.5em]
  \textbf{Jointly Gaussian:} ($\Theta, X \sim \mathcal{N})$
  \begin{align*}
    T_{CM} = E[\Theta \mid X = x] &= \underbar{\mu_\Theta} + \mat{C}_{\Theta X} \mat{C}_X^{-1} \underbrace{(\underbar{x} - \underbar{\mu}_X)}_{\Delta \underbar{x}}\\
    E[\mid\mid T_{CM} - \Theta \mid\mid^2] &= \text{tr}\{\mat{C}_{\Theta\mid X = x}\}
  \end{align*}
  \begin{align*}
    \mat{C}_{\Theta\mid X} = \mat{C}_\Theta - \mat{C}_{\Theta X} \mat{C}_X^{-1} \mat{C}_{X\Theta}\\
    \underbar{\mu}_{\Theta\mid X} = \underbar{\mu}_\Theta + \mat{C}_{\Theta X} \mat{C}_X^{-1} (\underbar{x} - \underbar{\mu}_X)
  \end{align*}
  \textbf{Orthogonality Principle:}
  \begin{align*}
    T_{CM}(X) - \Theta \perp h(X) \implies E[(T_{CM}(X) - \Theta)h(X)] = 0
  \end{align*}
  \small{$\implies$ Error has no correlation with the estimator or random variable}\\[0.5em]
  \textbf{Properties:}
  The CM Estimator is consistent, asymptotically unbiased and asymptotically efficient.
  The CM ist LMMSE if it is linear in $X$ e.q. the estimator is a linear function of $X$
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Comparison of Estimators}
  Estimate mean $\theta$ of $X$ with prior knowledge $\Theta \sim \mathcal{N}(\mu, \sigma_\Theta^2)$ and $X \sim \mathcal{N}(\theta, \sigma_X^2)$.\\[0.25em]
  For $N$ i.i.d. observations $x_i$
  \vspace*{-4pt}
  \begin{align*}
    \hat{\theta}_\text{CM} &= \frac{N \sigma_\Theta^2}{N \sigma_\Theta^2 + \sigma_{X\mid\theta}^2} \hat{\theta}_{\text{ML}} + \frac{\sigma_X^2}{N \sigma_\Theta^2 + \sigma_{X\mid\theta}^2} \mu\\
    \hat{\theta}_\text{ML} &= \frac{1}{N} \sum_{i=1}^N x_i
  \end{align*}
  For large $N$ or $\sigma_\Theta^2 \gg \sigma_{X\mid\theta}^2$ the ML estimator used.\\
  For small $N$ large $\sigma_{X\mid\theta}^2$ or small $\sigma_\Theta^2$ the knowledge about $\Theta$ is used to improve estimation.
\end{mdframed}