\begin{mdframed}[style=redbox]
  i.i.d: independent and identically distributed
\end{mdframed}

\section{Math Basics}
\begin{mdframed}[style=eqbox]
\subsection*{Binome, Trinome}
  \begin{align*}
    (a \pm b)^3 &= a^3 \pm 3a^2b + 3ab^2 \pm b^3 \\
    (a + b + c)^2 &= a^2 + b^2 + c^2 + 2ab + 2ac + 2bc
  \end{align*}
\subsection*{Sequences and Series}
  \begin{align*}
    \shortintertext{\scriptsize{~~(Aritmetric Series) ~~~~~~~~~ (Geometric Series) ~~~~~~~ (Exponential Series)}}
    \sum_{k=1}^n k = \frac{n(n+1)}{2} && \sum_{k = 0}^n q^k = \frac{1 - q^{n+1}}{1 - q} && \sum_{k = 0}^\infty \frac{z^k}{k!} = e^z
  \end{align*}
\subsection*{Mean}
\begin{align*}
  \mu_{ar} = \frac{1}{N} \sum x_i &\leq& \mu_{geo} = \sqrt[N]{\prod x_i} &\leq& \mu_{har} = \frac{N}{\sum \frac{1}{x_i}}
\end{align*}
% Ungleichungen (Englisch)
\subsection*{Inequalities}
Cauchy-Schwarz: $\mid \underbar{x}^T \underbar{y} \mid \leq \mid\mid \underbar{x} \mid\mid \cdot \mid\mid \underbar{y} \mid\mid$\\[0.5em]
Bernoulli: $(1 + x)^n \geq 1 + nx$\\[0.5em]
Triangle: $\mid a + b \mid \leq \mid a \mid + \mid b \mid$
\subsection*{Sets}
De Morgan's Laws: $\overline{A \cup B} = \overline{A} \cap \overline{B}$, $\overline{A \cap B} = \overline{A} \cup \overline{B}$
\end{mdframed}

\begin{mdframed}[style=eqbox]
\subsection{Differentiation ($\forall \lambda, \mu \in \mathbb{R}$)}
\begin{align*}
  (\lambda f(x) + \mu g(x))' &= \lambda f'(x) + \mu g'(x)\\[0.25em]
  (f(x) \cdot g(x))' &= f'(x) \cdot g(x) + f(x) \cdot g'(x)\\[0.25em]
  (f(g(x)))' &= f'(g(x)) \cdot g'(x)\\
  \left(\frac{f(x)}{g(x)}\right)' &= \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{g(x)^2}
\end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
\subsection{Integration}
\begin{align*}
  % Partielle Integration
  \int f'(x) \cdot g(x) \,dx &= f(x) \cdot g(x) - \int f(x) \cdot g'(x) \,dx\\[0.25em]
  % Substitution
  \int f(g(x)) \cdot g'(x) \,dx &= \int f(u) \,du \quad \text{mit } u = g(x)\\[0.25em]
\end{align*}
\end{mdframed}

\begin{mdframed}[style=bluebox]
% Table with important integrals and derivatives (width full page)
\begin{tabularx}{\textwidth}{XXX}
  \hline
  f(x) & F(x) - C & f'(x)\\
  \hline
  $x^n$ & $\frac{1}{n+1}x^{n+1}$ & $nx^{n-1}$\\
  $\log(ax)$ & $x\log(ax) - x$ & $\frac{1}{x}$\\
  $x \cdot e^x$ & $(x-1)e^x$ & $(x+1)e^x$\\
  $a^x$ & $\frac{a^x}{\log(a)}$ & $a^x \cdot \log(a)$\\
  $\sin(x)$ & $-\cos(x)$ & $\cos(x)$\\

  \hline
\end{tabularx}
\end{mdframed}

\begin{mdframed}[style=eqbox]
\subsection{Matrices}
$\mat{A} \in \mathbb{K}^{m \times n} $: Matrix with $m$ rows and $n$ columns
\begin{align*}
  (\mat{A} + \mat{B})^T &= \mat{A}^T + \mat{B}^T & (\mat{A} \cdot \mat{B})^T &= \mat{B}^T \cdot \mat{A}^T \\
  (\mat{A}^T)^{-1} &= (\mat{A}^{ - 1})^T & (\mat{A} \cdot \mat{B})^{-1} &= \mat{B}^{-1} \cdot \mat{A}^{-1}
\end{align*}
$\dim(\mat{A}) = n = \text{rank}(\mat{A}) + \dim \text{ker}(\mat{A})$

\subsubsection{Quadratic Matrices}
$\mat{A} \in \mathbb{K}^{n \times n}$: Square matrix of order $n$\\[0.25em]
regular/invertible/non-singular: $\det(\mat{A}) \neq 0$, $\text{rank}(\mat{A}) = n$\\
singular/non-invertible: $\det(\mat{A}) = 0$, $\text{rank}(\mat{A}) < n$\\
$\mat{A}^{-1}$ exists for regular matrices\\[0.25em]
orthogonal: $\mat{A}^T \cdot \mat{A} = \mat{A} \cdot \mat{A}^T = \mat{I} \implies \det(\mat{A}) = \pm 1$\\[0.25em]
symmetric: $\mat{A}^T = \mat{A}$

\subsubsection{Determinant of $\mat{A} \in \mathbb{K}^{n \times n}$}
$\det{\mat{A}} = \mid \mat{A} \mid$
\begin{align*}
  \det \left[\begin{array}{cc}
    \mat{A} & \mat{0} \\
    \mat{C} & \mat{D}
  \end{array}\right] &= \det \left[\begin{array}{cc}
    \mat{A} & \mat{B} \\
    \mat{0} & \mat{D}
  \end{array}\right] = \det(\mat{A}) \cdot \det(\mat{D})\\
\end{align*}
$\det{\mat{A}} = \det{\mat{A}^T}$\\
$\det{(\mat{A} \cdot \mat{B})} = \det{\mat{A}} \cdot \det{\mat{B}} = \det{\mat{B}} \cdot \det{\mat{A}} = \det{(\mat{B} \cdot \mat{A})}$\\[0.25em]
If $\text{rank}(\mat{A}) < n$, then $\det(\mat{A}) = 0$

\subsubsection{Eigenvalues and Eigenvectors}
\begin{align*}
  \mat{A} \cdot \underbar{x} = \lambda \cdot \underbar{x} && \det(\mat{A}) = \prod \lambda_i && \text{tr}\{\mat{A}\} = \sum \lambda_i\\
  &&\mat{A} = \mat{U} \cdot \mat{\Lambda} \cdot \mat{U}^{T}&&
\end{align*}
Eigenvectors of $\mat{A}$ span the range of $\mat{A}$\\
If only the trivial solution $\lambda = 0$ exists $\implies$ $\underbar{x} \in \text{ker}(\mat{A})$\\[0.25em]
EW of Triangular/Diagonal Matrix: $\lambda_i = a_{ii}$ (diagonal elements)

\subsubsection{Singularvalues and Singularvectors}
\begin{align*}
  \mat{A} = \mat{U} \cdot \mat{\Sigma} \cdot \mat{V}^{T} && \mat{A}^{T} \cdot \mat{A} = \mat{V} \cdot \mat{\Sigma}^{2} \cdot \mat{V}^{T} && \mat{A} \cdot \mat{A}^{T} = \mat{U} \cdot \mat{\Sigma}^{2} \cdot \mat{U}^{T}
\end{align*}
Left singular vectors span the range of $\mat{A}$\\
Right singular vectors span the range of $\mat{A}^{T}$ (domain of $\mat{A}$)\\
If $\sigma_i = 0$ then $\underbar{v}_i \in \text{ker}(\mat{A})$ and $\underbar{u}_i \in \text{ker}(\mat{A}^{T})$

\subsubsection{Helpful Tricks}
\vspace*{-10pt}
\begin{align*}
  tr(\mat{A}\mat{B}^T + \mat{B}\mat{A}^T) &= 2tr(\mat{A}\mat{B}^T)\\
  \underbar{a}^T\underbar{b} = \underbar{b}^T\underbar{a} &\implies \underbar{a}^T \mat{M} \underbar{b} &= \underbar{b}^T \mat{M}^T \underbar{a}
\end{align*}
\end{mdframed}

\newpage
\section{Probability Theory}
\begin{mdframed}[style=eqbox]
\subsection{Combinatorics}
Possible combinations/variations of choosing $k$ elements out of $n$ elements (distribute $k$ elements into $n$ bins):\\[0.25em]
\begin{tabularx}{\textwidth}{l|XX}
  \hline
  & with repetition & without repetition\\
  \hline
  order & $n^k$ & $\frac{n!}{(n-k)!}$\\[0.5em]
  no order & $\binom{n+k-1}{k}$ & $\binom{n}{k} = \frac{n!}{k!(n-k)!}$\\[0.5em]
  \hline
\end{tabularx}\vspace*{0.5em}
Permutations of $n$ elements: $n!$\\
Permutations of $n$ elements with $k$ same elements: $\frac{n!}{k_1! \cdot k_2! \cdot \ldots \cdot k_n!}$\\[0.25em]
Binomialcoefficient: $\binom{n}{k} = \binom{n}{n-k} = \frac{n!}{k!(n-k)!}$\\
\begin{align*}
  \binom{n}{0} = 1 & & \binom{n}{1} = n & & \binom{n}{n} = 1\\
\end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
\subsection{Probability space}
Sample space: Set of all possible outcomes\\
$\Omega = \{ \omega_1, \omega_2, \ldots, \omega_n \}$\\[0.25em]
Event space: Set of all possible events\\
$\mathbb{F} = \{ A_1, A_2, \ldots, A_n \}$ with $A_i \subseteq \Omega$\\[0.25em]
Probability measure: Assigns probabilities to events\\
$P : \mathbb{F} \rightarrow [0,1]$\\[0.25em]
Random variable: Maps outcomes to events\\
$X : \Omega \rightarrow \Omega$ with $X(\omega) = x \in A$\\[0.25em]
Observations: Single outcome of a random variable\\
$\{x_1, x_2, \ldots, x_n\} \subseteq \Omega$\\[0.25em]
Unknown parameters: Parameters of a probability distribution\\
$\theta \in \Theta$\\[0.25em]
Estimator: Function of observations that estimates $\theta$\\
$T: \mathbb{X} \rightarrow \Theta \implies \hat{\theta} = T(X)$
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Probability measure}
  \begin{align*}
    P(A) = \frac{\mid A \mid}{\mid \Omega \mid} && P(A \cup B) = P(A) + P(B) - P(A \cap B)\\
  \end{align*}
  The probability of the event $A$ is the number of outcomes in $A$ divided by the total number of outcomes in $\Omega$
  \subsubsection{Axioms of Kolmogorov}
  \begin{align*}
    \shortintertext{with $A_i \cap A_j = \emptyset$ for $i \neq j$}
    P(A) \geq 0 && P(\Omega) = 1 && P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)\\
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Distribution}
  Probabilitydensity function (PDF): \\
  $f_X(x) = \frac{\text{d} F_X(x)}{\text{d} x}$\\
  $f_{X,Y}(x,y) = \frac{\text{d}^2 F_{X,Y}(x,y)}{\text{d}x~\text{d}y}$ (Joint PDF)\\[0.5em]
  Cumulative distribution function (CDF): \\
  $F_X(x) = P({X \leq x}) = \int_{-\infty}^{x} f_X(t) \text{d} t$\\
  $F_{X,Y}(x,y) = P({X \leq x, Y \leq y}) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X,Y}(s,t) \text{d}s~\text{d}t$
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Conditional Probability}
  Probability of event $A$ given that event $B$ has occurred:
  \begin{align*}
    P(A \mid B) = \frac{P(A \cap B)}{P(B)} && P(A \cap B) = P(A \mid B) \cdot P(B)\\
  \end{align*}
  \vspace*{-32pt}
  \begin{align*}
    f_{X,Y}(x,y) &= f_{X \mid Y}(x \mid y) \cdot f_Y(y) = f_{Y \mid X}(y \mid x) \cdot f_X(x)\\
    f_{Y}(y) &= \underbrace{\int f_{X,Y}(x,y) \text{d}x}_{\text{marginalization}} = \int f_{Y \mid X}(y \mid x) \cdot f_X(x) \text{d}x\\
  \end{align*}\vspace*{-24pt}\\
  Bayes' Theorem:
  \vspace*{-4pt}
  \begin{align*}
    P(A \mid B) &= \frac{P(B \mid A) \cdot P(A)}{P(B)}\\
    f_{X \mid Y}(x \mid y) &= \frac{f_{X,Y}(x,y)}{f_Y(y)}
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Independence}
  $X_1, X_2, \ldots, X_n$ are independent if and only if:
  \vspace*{-6pt}
  \begin{align*}
    F_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} F_{X_i}(x_i)\\
    p_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} p_{X_i}(x_i)\\
    f_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} f_{X_i}(x_i)
  \end{align*}
\end{mdframed}

\section{Common Distributions}
\begin{mdframed}[style=eqbox]
  \subsection{Normal Distribution $\sim \mathcal{N}(\mu, \sigma^2)$}
  \vspace*{-6pt}
  \begin{align*}
    f_X(x) &= \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\\
    f_{\vec{X}}(\vec{x}) &= \frac{1}{\sqrt{(2 \pi)^k \det(\mat{C})}} \exp\left(-\frac{1}{2} (\vec{x}-\vec{\mu})^T \mat{C}^{-1} (\vec{x}-\vec{\mu})\right)\\[0.5em]
    \text{with~} &\det(a \mat{A}) = a^n \det(\mat{A}) \text{~if~} \mat{A} \in \mathbb{R}^{n \times n}
  \end{align*}
  \vspace*{-14pt}
  \begin{align*}
    E[X] = \mu && \text{Var}[X] = \sigma^2
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Uniform Distribution $\sim \mathcal{U}(a,b)$}
  \vspace*{-6pt}
  \begin{align*}
    f_X(x) = \frac{1}{b-a} && \mu = \frac{a+b}{2} && \sigma^2 = \frac{(b-a)^2}{12}
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Exponential Distribution $\sim \text{Exp}(\lambda)$}
  \vspace*{-6pt}
  \begin{align*}
    f_X(x) = \lambda \exp(-\lambda x) && \mu = \frac{1}{\lambda} && \sigma^2 = \frac{1}{\lambda^2}\\
    f_X(x; \theta) = \frac{h(x)\exp(a(\theta)t(x))}{\exp(b(\theta))}
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Gamma Distribution $\sim \Gamma(\alpha, \beta)$}
  \vspace*{-6pt}
  \begin{align*}
    f_X(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} \exp(-\beta x) && \mu = \frac{\alpha}{\beta} && \sigma^2 = \frac{\alpha}{\beta^2}
  \end{align*}
\end{mdframed}
\newpage
\begin{mdframed}[style=eqbox]
  \subsection{Binomial Distribution $\sim \mathcal{B}(K;\theta)$}
  \vspace*{-6pt}
  \begin{align*}
    f_X(x) = \binom{K}{x} \theta^x (1-\theta)^{K-x} && \mu = K \theta && \sigma^2 = K \theta (1-\theta)
  \end{align*}
\end{mdframed}

\section{Important Properties}
\begin{mdframed}[style=eqbox]
  \subsection{Expectation (first order moment)}
  The expectation of a random variable $X$ is the average value of $X$ over many trials.
  \begin{mdframed}[style=redbox]
    \vspace*{-11pt}
    \begin{align*}
      E[X] &= \int_{-\infty}^{\infty} x f_X(x) \text{d}x &\hat{ =}& \sum_{x \in \mathcal{X}} x \cdot P_X(x)\\
      E[g(X)] &= \int_{-\infty}^{\infty} g(x) f_X(x) \text{d}x &\hat{ =}& \sum_{x \in \mathcal{X}} g(x) \cdot P_X(x)
  \end{align*}
  \end{mdframed}
  \vspace*{-9pt}\begin{align*}
    E[aX + b] &= aE[X] + b\\
    E[X + Y] &= E[X] + E[Y]\\
    E[XY] &= E[X]E[Y] && \text{if~} X \text{~and~} Y \text{~are independent}\\
    E[g(X)] &= \int_{-\infty}^{\infty} g(x) f_X(x) \text{d}x
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Variance (second order moment)}
  The variance of a random variable $X$ is the average squared deviation from the mean.
  \begin{mdframed}[style=redbox]
    \vspace*{-11pt}
    \begin{align*}
      \text{Var}[X] &= E[(X - E[X])^2] = E[X^2] - E[X]^2\\
      \text{Var}[\underbar{X}] &= \text{E}[(\underbar{X} - \vec{\mu})(\underbar{X} - \vec{\mu})^T] = \text{E}[\underbar{X}\underbar{X}^T] - \vec{\mu}\vec{\mu}^T
    \end{align*}
  \end{mdframed}
  \vspace*{-9pt}\begin{align*}
    \text{Var}[X] &= \text{Cov}[X,X]\\
    \text{Var}[aX + b] &= a^2 \text{Var}[X]\\
    \text{Var}[X + Y] &= \text{Var}[X] + \text{Var}[Y] + 2\text{Cov}[X,Y]\\
    \text{Var}[XY] &= E[X^2]E[Y^2] - E[X]^2E[Y]^2 + \text{Cov}[X,Y]^2\\
    \text{Var}[X] &= E[X^2] - E[X]^2\\
    \text{Var}\left[\sum_{i=1}^n X_i\right] &= \sum_{i=1}^n \text{Var}[X_i] + \sum_{i \neq j} \text{Cov}[X_i, X_j]
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Covariance}
  The covariance of two random variables $X$ and $Y$ is a measure of how much one can be expressed by the other.
  \begin{mdframed}[style=redbox]
    \vspace*{-11pt}
    \begin{align*}
      \text{Cov}[X,Y] &= E[(X-\mu_X)(Y-\mu_Y)] = E[XY] - \mu_X\mu_Y\\
      \text{Cov}[\underbar{X},\underbar{Y}] &= \text{E}[(\underbar{X} - \vec{\mu}_X)(\underbar{Y} - \vec{\mu}_Y)^T]
    \end{align*}
  \end{mdframed}
  \vspace*{-9pt}\begin{align*}
    \text{Cov}[X,Y] &= \text{Cov}[Y,X]\\
    \text{Cov}[aX + b, cY + d] &= ac \text{Cov}[X,Y]\\
    \text{Cov}[X + U, Y] &= \text{Cov}[X,Y] + \text{Cov}[U,Y]\\
    \text{Cov}[\underbar{z}] = \mat{C}_{\underbar{z}} &= \begin{bmatrix}
      \mat{C}_{\underbar{x}} & \mat{C}_{\underbar{x},\underbar{y}}\\
      \mat{C}_{\underbar{y},\underbar{x}} & \mat{C}_{\underbar{y}}
    \end{bmatrix} \text{~with~} \underbar{z} = [\underbar{x}^T, \underbar{y}^T]^T
  \end{align*}
  \vspace*{-20pt}\subsubsection{Correlation}
  The correlation is the normalized covariance.
  \begin{align*}
    \rho(X,Y) &= \frac{\text{Cov}[X,Y]}{\sqrt{\text{Var}[X]\text{Var}[Y]}} = \frac{\mat{C}_{X,Y}}{\sigma_X \sigma_Y}
  \end{align*}
\end{mdframed}

\section{Estimation}
\begin{mdframed}[style=eqbox]
  \subsection{Estimator Quality}
  Consistency: $\lim_{n \rightarrow \infty} P(\mid \hat{\theta}_n - \theta \mid > \epsilon) = 0$\\
  Unbiasedness: $E[\hat{\theta}] = \theta$ with $\text{Bias}[\hat{\theta}] = E[\hat{\theta}] - \theta$\\
  Variance: $\text{Var}[T] = E[(T - E[T])^2]$
  \subsubsection{Mean Squared Error (MSE)}
  The MSE is the expected value of the squared error.
  \begin{mdframed}[style=redbox]
    \vspace*{-10pt}
    \begin{align*}
      \epsilon[T] &= E[(T - \theta)^2] = \text{Var}[T] + \text{Bias}^2[T]\\
      \epsilon[\underbar{T}] &= \text{E}[\mid\mid \underbar{T} - \underbar{\theta} \mid\mid^2] = \text{tr}\{E[(\underbar{T} - \underbar{\theta})(\underbar{T} - \underbar{\theta})^T]\}
    \end{align*}
  \end{mdframed}
  \subsubsection{Minimum Mean Squared Error (MMSE)}
  The MMSE is the minimum MSE over all possible estimators.
  \begin{align*}
    \arg \min_T E[(T - \theta)^2]
  \end{align*}
\end{mdframed}

\begin{mdframed}[style=eqbox]
  \subsection{Maximum Likelihood Estimation (ML)}
  The ML estimator is the value of $\theta$ that maximizes the likelihood function $L(x;\theta)$ given $f_X(x; \theta)$.\\[0.25em]
  \textbf{Likelihood function:}\\
  $L(x_1, \ldots, x_n; \theta) = f_{X_1, \ldots, X_n}(x_1, \ldots, x_n; \theta)$\\
  $L(x_1, \ldots, x_n; \theta) = P_\theta(X_1 = x_1, \ldots, X_n = x_n)$\\[0.5em]
  If $N$ observations are i.i.d.:
  \vspace*{-4pt}
  \begin{align*}
    L(x;\theta) = \prod_{i=1}^N f_{X_i}(x_i; \theta) && l(x;\theta) = \sum_{i=1}^N \log f_{X_i}(x_i; \theta)
  \end{align*}
  \textbf{Maximum Likelihood Estimator:}
  \vspace*{-4pt}
  \begin{align*}
    T_{ML} = \arg \max_\theta \{ L(x;\theta) \} &= \arg\max_\theta \{ l(x;\theta)\}\\
    \frac{\delta L(x;\theta)}{\delta \theta} &= \frac{\delta l(x;\theta)}{\delta \theta} \overset{!}{ =} 0
  \end{align*}
  \textbf{Properties:}
  The ML Estimator is consistent, asymptotically unbiased and asymptotically efficient.
\end{mdframed}
\newpage
% Uniformly Minimum Variance Unbiased Estimator (UMVU)
\begin{mdframed}[style=eqbox]
  \subsection{Uniformly Minimum Variance Unbiased Estimator (UMVU)}
  The UMVU estimator is the unbiased estimator with the smallest variance. (Best unbiased estimator)\\[0.25em]
  \textbf{Fisher's Information Inequality:} Estimate lower bound for the variance if\\
  $ L(x, \theta) > 0 \quad \forall x, \theta$\\[0.25em]
  $ L(x, \theta)$ is twice differentiable in $\theta$\\[0.25em]
  $ \frac{\delta}{\delta \theta} \int L(x, \theta) \text{d}x = \int \frac{\delta}{\delta \theta} L(x, \theta) \text{d}x$\\[0.5em]
  \textbf{Score function:}
  \vspace*{-4pt}
  \begin{align*}
    g(x, \theta) = \frac{\delta}{\delta \theta} \log L(x, \theta) = \frac{\frac{\delta}{\delta \theta} L(x, \theta)}{L(x, \theta)} && E[g(x, \theta)] = 0
  \end{align*}
  \textbf{Fisher information:}
  \vspace*{-4pt}
  \begin{align*}
    I_F(\theta) := Var[g(X, \theta)] = E[g(x, \theta)^2] = -E\left[\frac{\delta^2}{\delta \theta^2} \log L(x, \theta)\right]
  \end{align*}
  \textbf{Cramer-Rao Lower Bound (CRB):}
  \vspace*{-4pt}
  \begin{mdframed}[style=redbox]
    \vspace*{-10pt}
    \begin{align*}
      \text{Var}[T] \geq \left ( \frac{\delta E[T(X)]}{\delta \theta} \right )^2 \frac{1}{I_F(\theta)} && \text{Var}[T] \geq \frac{1}{I_F(\theta)}
    \end{align*}
  \end{mdframed}
  \vspace*{-4pt}\small{with $T$ being unbiased $\implies E[T(X)] = \theta$}\\[0.25em]
  For $N$ i.i.d. observations: $I_F^{(N)}(x, \theta) = N \cdot I_F(x, \theta)$
  \subsubsection{Exponential Models}
  \vspace*{-4pt}
  \begin{align*}
    f_X(x; \theta) = \frac{h(x)\exp(a(\theta)t(x))}{\exp(b(\theta))} && I_F(\theta) = \frac{\delta a}{\delta \theta} \frac{\delta E[t(X)]}{\delta \theta}
  \end{align*}
  \subsubsection{Useful derivations}
  Uniform $\mathcal{U}(a,b)$: Not differentiable $\implies$ no $I_F(\theta)$\\[0.25em]
  Normal $\mathcal{N}(\mu, \sigma^2)$: $g(x,\theta) = \frac{x - \theta}{\sigma^2}$ $I_F(\theta) = \frac{1}{\sigma^2}$\\[0.25em]
  Binomial $\mathcal{B}(K, \theta)$: $g(x,\theta) = \frac{x}{\theta} - \frac{K-x}{1-\theta}$ $I_F(\theta) = \frac{K}{\theta(1-\theta)}$\\[0.25em]
\end{mdframed}

% Bayes Estimation (Conditional Mean)
